{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import onmt\n",
    "import onmt.io\n",
    "import onmt.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading in the vocabulary for the model of interest. This will let us check vocab size and to get the special ids for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict(torch.load(\"../../data/data.vocab.pt\"))\n",
    "src_padding = vocab[\"src\"].stoi[onmt.io.PAD_WORD]\n",
    "tgt_padding = vocab[\"tgt\"].stoi[onmt.io.PAD_WORD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify the core model itself. Here we will build a small model with an encoder and an attention based input feeding decoder. Both models will be RNNs and the encoder will be bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 10\n",
    "rnn_size = 6\n",
    "# Specify the core model. \n",
    "encoder_embeddings = onmt.modules.Embeddings(emb_size, len(vocab[\"src\"]),\n",
    "                                             word_padding_idx=src_padding)\n",
    "\n",
    "encoder = onmt.modules.RNNEncoder(hidden_size=rnn_size, num_layers=1, \n",
    "                                 rnn_type=\"LSTM\", bidirectional=True,\n",
    "                                 embeddings=encoder_embeddings)\n",
    "\n",
    "decoder_embeddings = onmt.modules.Embeddings(emb_size, len(vocab[\"tgt\"]),\n",
    "                                            word_padding_idx=tgt_padding)\n",
    "decoder = onmt.modules.InputFeedRNNDecoder(hidden_size=rnn_size, num_layers=1, \n",
    "                                           bidirectional_encoder=True,\n",
    "                                           rnn_type=\"LSTM\", embeddings=decoder_embeddings)\n",
    "\n",
    "model = onmt.modules.NMTModel(encoder, decoder)\n",
    "\n",
    "# Specify the tgt word generator and loss computation module\n",
    "model.generator = nn.Sequential(                                                                                                                        \n",
    "            nn.Linear(rnn_size, len(vocab[\"tgt\"])),                                                                                           \n",
    "            nn.LogSoftmax())\n",
    "loss = onmt.Loss.NMTLossCompute(model.generator, vocab[\"tgt\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up the optimizer. This could be a core torch optim class, or our wrapper which handles learning rate updates and gradient normalization automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = onmt.Optim(method=\"sgd\", lr=1, max_grad_norm=2)\n",
    "optim.set_parameters(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the data from disk. Currently will need to call a function to load the fields into the data as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some data\n",
    "data = torch.load(\"../../data/data.train.pt\")\n",
    "valid_data = torch.load(\"../../data/data.valid.pt\")\n",
    "data.examples = data.examples[:100]\n",
    "                                        \n",
    "# TODO: This is a bit hacky, need to clean up                                                                                                                       \n",
    "fields = onmt.io.load_fields_from_vocab(vocab.items(), \"text\")\n",
    "fields = dict([(k, f) for (k, f) in fields.items()                                                                                                         \n",
    "                  if k in data.examples[0].__dict__])\n",
    "data.fields = valid_data.fields = fields                                                                           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To iterate through the data itself we use a torchtext iterator class. We specify one for both the training and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = onmt.io.OrderedIterator(                                                                                                                            \n",
    "                dataset=data, batch_size=10, \n",
    "                device=-1,                                                                                                                                                                                 \n",
    "                repeat=False)\n",
    "valid_iter = onmt.io.OrderedIterator(                                                                                                                            \n",
    "                dataset=valid_data, batch_size=10,                                                                                                                                                                                       \n",
    "                device=-1,\n",
    "                train=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0,     0/   10; acc:   0.00; ppl: 957.49; 1272 src tok/s; 1212 tgt tok/s; 1514089083 s elapsed\n",
      "Epoch  0,     1/   10; acc:  19.42; ppl: 650.16; 1369 src tok/s; 1406 tgt tok/s; 1514089083 s elapsed\n",
      "Epoch  0,     2/   10; acc:  22.40; ppl: 526.48; 1344 src tok/s; 1369 tgt tok/s; 1514089083 s elapsed\n",
      "Epoch  0,     3/   10; acc:  24.31; ppl: 422.44; 1431 src tok/s; 1409 tgt tok/s; 1514089084 s elapsed\n",
      "Epoch  0,     4/   10; acc:  25.30; ppl: 379.79; 1430 src tok/s; 1431 tgt tok/s; 1514089084 s elapsed\n",
      "Epoch  0,     5/   10; acc:  26.38; ppl: 333.62; 1400 src tok/s; 1398 tgt tok/s; 1514089084 s elapsed\n",
      "Epoch  0,     6/   10; acc:  27.15; ppl: 299.44; 1451 src tok/s; 1447 tgt tok/s; 1514089084 s elapsed\n",
      "Epoch  0,     7/   10; acc:  27.63; ppl: 262.90; 1564 src tok/s; 1505 tgt tok/s; 1514089084 s elapsed\n",
      "Epoch  0,     8/   10; acc:  27.67; ppl: 254.80; 1575 src tok/s; 1524 tgt tok/s; 1514089084 s elapsed\n",
      "Epoch  0,     9/   10; acc:  28.37; ppl: 232.36; 1618 src tok/s; 1564 tgt tok/s; 1514089085 s elapsed\n",
      "Validation\n",
      "Epoch  0,    11/   10; acc:  16.55; ppl: 119.81;   0 src tok/s; 7456 tgt tok/s; 1514089093 s elapsed\n",
      "Epoch  1,     0/   10; acc:  23.91; ppl: 171.31; 1608 src tok/s; 1712 tgt tok/s; 1514089093 s elapsed\n",
      "Epoch  1,     1/   10; acc:  27.45; ppl: 132.93; 1737 src tok/s; 1585 tgt tok/s; 1514089094 s elapsed\n",
      "Epoch  1,     2/   10; acc:  22.04; ppl: 134.41; 1692 src tok/s; 1587 tgt tok/s; 1514089094 s elapsed\n",
      "Epoch  1,     3/   10; acc:  25.29; ppl: 118.76; 1635 src tok/s; 1543 tgt tok/s; 1514089094 s elapsed\n",
      "Epoch  1,     4/   10; acc:  22.72; ppl: 117.48; 1630 src tok/s; 1579 tgt tok/s; 1514089094 s elapsed\n",
      "Epoch  1,     5/   10; acc:  24.05; ppl: 115.13; 1704 src tok/s; 1654 tgt tok/s; 1514089094 s elapsed\n",
      "Epoch  1,     6/   10; acc:  22.45; ppl: 111.95; 1680 src tok/s; 1639 tgt tok/s; 1514089094 s elapsed\n",
      "Epoch  1,     7/   10; acc:  23.37; ppl: 110.24; 1713 src tok/s; 1663 tgt tok/s; 1514089094 s elapsed\n",
      "Epoch  1,     8/   10; acc:  22.33; ppl: 110.86; 1719 src tok/s; 1661 tgt tok/s; 1514089095 s elapsed\n",
      "Epoch  1,     9/   10; acc:  22.71; ppl: 109.56; 1705 src tok/s; 1649 tgt tok/s; 1514089095 s elapsed\n",
      "Validation\n",
      "Epoch  1,    11/   10; acc:  14.96; ppl:  95.72;   0 src tok/s; 6934 tgt tok/s; 1514089104 s elapsed\n"
     ]
    }
   ],
   "source": [
    "trainer = onmt.Trainer(model, train_iter, valid_iter,                                                                                                      \n",
    "                       loss, loss, optim)\n",
    "\n",
    "def report_func(*args):\n",
    "    stats = args[-1]\n",
    "    stats.output(args[0], args[1], 10, 0)\n",
    "    return stats\n",
    "\n",
    "for epoch in range(2):\n",
    "    trainer.train(epoch, report_func)\n",
    "    val_stats = trainer.validate()\n",
    "    print(\"Validation\")\n",
    "    val_stats.output(epoch, 11, 10, 0)\n",
    "    trainer.epoch_step(val_stats.ppl(), epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model, we need to load up the translation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import onmt.translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.src_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/tensor.py:297: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  return self.add_(other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED SCORE: -2.4383\n",
      "\n",
      "SENT 0: ('The', 'competitors', 'have', 'other', 'advantages', ',', 'too', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.4250\n",
      "\n",
      "SENT 0: ('The', 'company', '&apos;s', 'durability', 'goes', 'back', 'to', 'its', 'first', 'boss', ',', 'a', 'visionary', ',', 'Thomas', 'J.', 'Watson', 'Sr.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.3067\n",
      "\n",
      "SENT 0: ('&quot;', 'From', 'what', 'we', 'know', 'today', ',', 'you', 'have', 'to', 'ask', 'how', 'I', 'could', 'be', 'so', 'wrong', '.', '&quot;')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.3162\n",
      "\n",
      "SENT 0: ('Boeing', 'Co', 'shares', 'rose', '1.5%', 'to', '$', '67.94', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.3477\n",
      "\n",
      "SENT 0: ('Some', 'did', 'not', 'believe', 'him', ',', 'they', 'said', 'that', 'he', 'got', 'dizzy', 'even', 'in', 'the', 'truck', ',', 'but', 'always', 'wanted', 'to', 'fulfill', 'his', 'dream', ',', 'that', 'of', 'becoming', 'a', 'pilot', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.3305\n",
      "\n",
      "SENT 0: ('In', 'your', 'opinion', ',', 'the', 'council', 'should', 'ensure', 'that', 'the', 'band', 'immediately', 'above', 'the', 'Ronda', 'de', 'Dalt', 'should', 'provide', 'in', 'its', 'entirety', ',', 'an', 'area', 'of', 'equipment', 'to', 'conduct', 'a', 'smooth', 'transition', 'between', 'the', 'city', 'and', 'the', 'green', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.3389\n",
      "\n",
      "SENT 0: ('The', 'clerk', 'of', 'the', 'court', ',', 'Jorge', 'Yanez', ',', 'went', 'to', 'the', 'jail', 'of', 'the', 'municipality', 'of', 'San', 'Nicolas', 'of', 'Garza', 'to', 'notify', 'Jonah', 'that', 'he', 'has', 'been', 'legally', 'pardoned', 'and', 'his', 'record', 'will', 'be', 'filed', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.0094\n",
      "\n",
      "SENT 0: ('&quot;', 'In', 'a', 'research', 'it', 'is', 'reported', 'that', 'there', 'are', 'no', 'parts', 'or', 'components', 'of', 'the', 'ship', 'in', 'another', 'place', ',', 'the', 'impact', 'is', 'presented', 'in', 'a', 'structural', 'way', '.')\n",
      "PRED 0: \n",
      "\n",
      "PRED SCORE: -1.3840\n",
      "\n",
      "SENT 0: ('On', 'the', 'asphalt', 'covering', ',', 'he', 'added', ',', 'is', 'placed', 'a', 'final', 'layer', 'called', 'rolling', 'covering', ',', 'which', 'is', 'made', '\\u200b', '\\u200b', 'of', 'a', 'fine', 'stone', 'material', ',', 'meaning', 'sand', 'also', 'dipped', 'into', 'the', 'asphalt', '.')\n",
      "PRED 0: \n",
      "\n",
      "PRED SCORE: -2.3496\n",
      "\n",
      "SENT 0: ('This', 'is', '200', 'bar', 'on', 'leaving', 'and', '100', 'bar', 'on', 'arrival', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.5771\n",
      "\n",
      "SENT 0: ('&quot;', 'Darina', 'and', 'I', 'communicate', 'in', 'a', 'completely', 'normal', 'way', ',', 'only', 'we', 'do', 'not', 'share', 'our', 'bed', 'anymore', ',', '&quot;', 'Rychtář', 'said', 'for', 'Blesk', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.2918\n",
      "\n",
      "SENT 0: ('All', 'this', 'under', 'the', 'strict', 'supervision', 'of', 'the', 'monument', 'authority', 'and', 'with', 'a', 'limited', 'budget', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.3818\n",
      "\n",
      "SENT 0: ('A', 'German', 'is', 'now', 'to', 'bring', 'it', 'back', 'in', 'line', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.3407\n",
      "\n",
      "SENT 0: ('It', 'was', 'precisely', 'the', 'concept', 'of', 'caution', 'that', 'had', 'long', 'ago', 'made', 'Swiss', 'bankers', 'the', 'guardians', 'of', 'the', 'world', '&apos;s', 'major', 'assets', '-', 'no', 'excessive', 'risk', ',', 'but', 'security', 'for', 'money', ',', 'whether', 'the', 'world', 'fell', 'apart', 'outside', 'the', 'Alpine', 'republic', 'or', 'not', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.4257\n",
      "\n",
      "SENT 0: ('The', 'intelligence', 'and', 'skill', 'of', 'the', 'birds', 'are', 'legendary', '.', 'Rooks', 'often', 'do', 'not', 'just', 'use', 'tools', 'to', 'get', 'hold', 'of', 'a', 'choice', 'morsel', ',', 'but', 'create', 'tools', 'themselves', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -1.5253\n",
      "\n",
      "SENT 0: ('An', 'amendment', 'prohibiting', 'same-sex', 'marriage', 'is', 'a', 'perennial', 'offering', ',', 'as', 'is', 'one', 'prohibiting', 'flag-burning', '.')\n",
      "PRED 0: \n",
      "\n",
      "PRED SCORE: -2.3219\n",
      "\n",
      "SENT 0: ('Don', '&apos;t', 'conclude', 'from', 'this', 'that', 'liars', 'are', 'hard', 'to', 'spot', 'and', 'difficult', 'to', 'unmask', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.2758\n",
      "\n",
      "SENT 0: ('For', 'Costa', 'Rica', ',', 'Bryan', 'Ruiz', 'is', 'the', 'one', 'that', 'registers', 'more', 'points', ',', 'two', 'more', 'than', 'Carlos', 'Hernandez', 'and', 'three', 'more', 'than', 'Roy', 'Myrie', 'and', 'Parks', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.3269\n",
      "\n",
      "SENT 0: ('We', 'have', 'many', 'friends', 'who', 'are', 'surfers', 'and', 'I', 'have', 'a', 'great', 'friend', 'who', 'lives', 'in', 'Tamarindo', 'and', 'the', 'waves', 'here', 'are', 'incredible', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.4600\n",
      "\n",
      "SENT 0: ('On', 'Monday', ',', 'the', 'legal', 'representative', 'of', 'Red', 'Casino', ',', 'Victor', 'Aldo', 'Garcia', 'Gomez', ',', 'appeared', 'before', 'the', 'judge', 'José', 'Luis', 'Pecina', 'to', 'seek', 'pardon', 'for', 'the', 'regional', 'mayor', '&apos;s', 'brother', ',', 'so', 'that', 'in', 'the', 'coming', 'hours', 'he', 'might', 'regain', 'his', 'freedom', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.3841\n",
      "\n",
      "SENT 0: ('&quot;', 'Yet', ',', 'I', 'don', '&apos;t', 'see', 'a', 'point', 'in', 'transferring', 'my', 'shares', 'in', 'the', 'Pilsen', 'club', 'hypocritically', 'to', 'someone', 'else', ',', 'while', 'coming', 'back', 'there', 'in', 'six', 'months', ',', '&quot;', 'Paclík', 'says', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.3921\n",
      "\n",
      "SENT 0: ('The', 'police', 'announced', 'in', 'a', 'statement', 'that', 'he', 'had', 'threatened', 'a', '25-year-old', 'employee', 'with', 'a', 'firearm', 'and', 'demanded', 'money', 'be', 'handed', 'over', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -1.5465\n",
      "\n",
      "SENT 0: ('The', 'troops', 'are', 'lavished', 'with', 'praise', 'for', 'their', 'sacrifices', '.')\n",
      "PRED 0: \n",
      "\n",
      "PRED SCORE: -2.3463\n",
      "\n",
      "SENT 0: ('After', 'a', 'brief', 'rest', 'at', 'the', 'end', 'of', 'last', 'week', ',', 'when', 'it', 'became', 'clear', 'that', 'Berlusconi', 'would', 'resign', ',', 'the', 'Italian', 'debt', 'costs', 'have', 'now', 'reached', 'critical', 'levels', 'between', 'uncertainties', 'about', 'whether', 'the', 'new', 'prime', 'minister', 'will', 'succeed', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.4714\n",
      "\n",
      "SENT 0: ('Regarding', 'persons', 'who', 'commit', 'assisted', 'suicide', ',', 'investigations', 'indicate', 'that', '41%', 'of', 'such', 'cases', 'should', 'not', 'have', 'been', 'prosecuted', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.2740\n",
      "\n",
      "SENT 0: ('Damascus', 'denounces', 'a', '&quot;', 'plot', '&quot;')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.3836\n",
      "\n",
      "SENT 0: ('&quot;', 'Human', 'Rights', 'First', 'has', 'long', 'stressed', 'that', 'acts', 'of', 'violence', 'against', 'Moslems', ',', 'as', 'well', 'as', 'all', 'forms', 'of', 'hate', 'crime', ',', 'must', 'be', 'regarded', 'as', 'serious', 'violations', 'of', 'human', 'rights', ',', '&quot;', 'adds', 'one', 'of', 'the', 'managers', 'of', 'the', 'organisation', ',', 'Paul', 'Legendre', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.3782\n",
      "\n",
      "SENT 0: ('If', 'France', 'decided', 'to', 'abandon', 'nuclear', 'power', ',', 'she', 'would', 'sacrifice', 'a', 'large', 'number', 'of', 'these', 'RME', '&apos;s', 'and', 'lose', 'the', 'annual', '6', 'thousand', 'million', 'Euro', '&apos;s', 'worth', 'of', 'French', 'exports', 'of', 'nuclear', 'equipment', 'and', 'services', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.3774\n",
      "\n",
      "SENT 0: ('These', 'two', 'countries', 'will', 'lose', 'some', 'of', 'their', 'power:', 'it', 'will', 'become', 'more', 'difficult', 'for', 'them', 'to', 'influence', 'Russia', 'by', 'threatening', 'to', 'close', 'access', 'to', 'West', 'European', 'markets', '.')\n",
      "PRED 0: <unk>\n",
      "\n",
      "PRED SCORE: -2.2784\n",
      "\n",
      "SENT 0: ('&quot;', 'We', 'are', 'expecting', 'a', 'very', 'close', 'match', ',', 'but', 'we', 'are', 'convinced', 'that', 'we', 'can', 'win', ',', 'declared', 'the', 'Portuguese', 'trainer', 'aged', '42', '.')\n",
      "PRED 0: <unk>\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ec5f8662dc2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrans_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtranslations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrans\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranslations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/onmt/translate/Translation.py\u001b[0m in \u001b[0;36mfrom_batch\u001b[0;34m(self, translation_batch)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0msrc_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_vocabs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                   \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_vocabs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0msrc_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0msrc_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "translator = onmt.translate.Translator(beam_size=10, fields=fields, model=model)\n",
    "builder = onmt.translate.TranslationBuilder(data=valid_data, fields=fields)\n",
    "\n",
    "valid_data.src_vocabs\n",
    "for batch in valid_iter:\n",
    "    trans_batch = translator.translate_batch(batch=batch, data=valid_data)\n",
    "    translations = builder.from_batch(trans_batch)\n",
    "    for trans in translations:\n",
    "        print(trans.log(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
